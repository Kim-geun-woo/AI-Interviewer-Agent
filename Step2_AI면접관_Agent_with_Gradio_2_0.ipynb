{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step2 제공파일_AI면접관 Agent_with Gradio**"
      ],
      "metadata": {
        "id": "mSB2IiVH8B1v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU-xxYwejwGR"
      },
      "source": [
        "## **1. 환경준비**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdcBWhy_F_Hm"
      },
      "source": [
        "### (1) 구글 드라이브"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) 구글 드라이브 폴더 생성\n",
        "* 새 폴더(project_genai)를 생성하고\n",
        "* 제공 받은 파일을 업로드"
      ],
      "metadata": {
        "id": "xUOpvAJGGJnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 구글 드라이브 연결"
      ],
      "metadata": {
        "id": "4jUC5td4GLEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tEfLUT6ZGEJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8293d844-dc76-4a54-eecb-4b0ec064edec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) 라이브러리"
      ],
      "metadata": {
        "id": "PepxmQuiGzkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain_core langchain-community -q\n",
        "!pip install -U langgraph\n",
        "!pip install PyMuPDF python-docx gradio -q"
      ],
      "metadata": {
        "id": "TwO3_Qx4PlM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5df573-a1c0-4785-ca3e-3d9c0e40f7af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.2 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langgraph\n",
            "  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.2)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-1.0.2-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-1.0.2 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 ormsgpack-1.11.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS5BhycUFUMI"
      },
      "source": [
        "### (3) OpenAI API Key 확인\n",
        "* api_key.txt 파일에 다음의 키를 등록하세요.\n",
        "    * OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "path = '/content/drive/MyDrive/project_genai/'\n",
        "# API 키 로드 및 환경변수 설정\n",
        "load_api_keys(path + 'api_key.txt')"
      ],
      "metadata": {
        "id": "AaZBGfeWNMRE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ['OPENAI_API_KEY'][:30])"
      ],
      "metadata": {
        "id": "GqSUhiv8wKxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. App.py**\n",
        "\n",
        "* 아래 코드에, Step1 혹은 고도화 된 Step2 파일 코드를 붙인다.\n",
        "    * 라이브러리\n",
        "    * 함수들과 그래프\n",
        "* Gradio 코드는 그대로 사용하거나 일부 수정 가능"
      ],
      "metadata": {
        "id": "ULAOaRHmShq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%writefile app.py  \n",
        "\n",
        "밑에 나오는 코드를 app.py에 저장한다는 의미"
      ],
      "metadata": {
        "id": "Jc51MvvpJdp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "####### 1. 라이브러리 로딩 #######\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import openai\n",
        "import random\n",
        "import ast\n",
        "import fitz  # PyMuPDF\n",
        "from docx import Document\n",
        "import json\n",
        "import warnings\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Gradio\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "from typing import Annotated, Literal, Sequence, TypedDict, List, Dict\n",
        "\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "####### 2. API 키 로딩 #######\n",
        "def load_api_keys(filepath=\"api_key.txt\"):\n",
        "    colab_path = '/content/drive/MyDrive/project_genai/api_key.txt'\n",
        "    local_path = 'api_key.txt'\n",
        "\n",
        "    if os.path.exists(colab_path):\n",
        "        key_path = colab_path\n",
        "    elif os.path.exists(local_path):\n",
        "        key_path = local_path\n",
        "    else:\n",
        "        print(\"API Key 파일을 찾을 수 없습니다. OPENAI_API_KEY 환경 변수를 직접 설정하세요.\")\n",
        "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "             raise ValueError(\"OPENAI_API_KEY가 설정되지 않았습니다.\")\n",
        "        return\n",
        "\n",
        "    with open(key_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and \"=\" in line:\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                os.environ[key.strip()] = value.strip()\n",
        "\n",
        "try:\n",
        "    load_api_keys()\n",
        "    if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        print(\"OpenAI API Key 로드 성공.\")\n",
        "    else:\n",
        "        print(\"API Key 파일에서 키를 찾지 못했습니다. 환경 변수를 확인하세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"API Key 로딩 중 오류 발생: {e}\")\n",
        "\n",
        "\n",
        "####### 3. 핵심 함수 및 Agent 로직 (팀과제 기반) #######\n",
        "\n",
        "DEBUG_MODE = False  # ⚙️ True면 디버깅 로그 출력\n",
        "\n",
        "# 1) 파일 입력\n",
        "def extract_text_from_file(file_path: str) -> str:\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == \".pdf\":\n",
        "        doc = fitz.open(file_path)\n",
        "        text = \"\\n\".join(page.get_text() for page in doc)\n",
        "        doc.close()\n",
        "        return text\n",
        "    elif ext == \".docx\":\n",
        "        doc = Document(file_path)\n",
        "        return \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
        "    else:\n",
        "        raise ValueError(\"지원하지 않는 파일 형식입니다. PDF 또는 DOCX만 허용됩니다.\")\n",
        "\n",
        "# 2) State 선언 (고도화 버전)\n",
        "class InterviewState(TypedDict):\n",
        "    resume_text: str\n",
        "    target_company:str\n",
        "    resume_summary: str\n",
        "    resume_keywords: List[str]\n",
        "    target_company_keywords: List[str]\n",
        "    question_strategy: Dict[str, Dict]\n",
        "    current_question: str\n",
        "    current_answer: str\n",
        "    current_strategy: str\n",
        "    conversation: List[Dict[str, str]]\n",
        "    evaluation : List[Dict[str, str]]\n",
        "    next_step : str\n",
        "    reflection_status: str\n",
        "    reflection_detail: Dict[str, str]\n",
        "    last_evaluation: Dict[str, str]\n",
        "    generate_count: str\n",
        "    final_summary: str\n",
        "    question_queue: List[str]\n",
        "    remaining_topics: List[str]\n",
        "    topic_map: Dict[str, List]\n",
        "\n",
        "# 3) Resume 분석 (고도화 버전)\n",
        "def analyze_resume(state: InterviewState) -> InterviewState:\n",
        "    resume_text = state.get('resume_text')\n",
        "    target_company = state.get('target_company')\n",
        "\n",
        "    if not resume_text or resume_text.strip() == \"\":\n",
        "        return {**state, \"next_step\": \"end\", \"final_summary\": \"오류: 이력서 텍스트가 없습니다.\"}\n",
        "    if not target_company or target_company.strip() == \"\":\n",
        "        return {**state, \"next_step\": \"end\", \"final_summary\": \"오류: 채용공고 텍스트가 없습니다.\"}\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    keywords_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    당신은 채용담당자입니다. 이력서와 채용공고를 분석하여 핵심 요약과 키워드를 JSON으로 반환하세요.\n",
        "    [이력서]\n",
        "    {resume_text}\n",
        "    [채용공고]\n",
        "    {target_company}\n",
        "    [출력 형식]\n",
        "    {{\n",
        "      \"resume_summary\": \"이력서 핵심 요약 (3~4문장)\",\n",
        "      \"resume_keywords\": [\"키워드1\", ..., \"키워드10\"],\n",
        "      \"target_company_keywords\": [\"키워드1\", ..., \"키워드10\"]\n",
        "    }}\n",
        "    \"\"\")\n",
        "    keywords_chain = keywords_prompt | llm | JsonOutputParser()\n",
        "    try:\n",
        "        result = keywords_chain.invoke({\n",
        "            \"resume_text\": resume_text,\n",
        "            \"target_company\": target_company\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return {**state, \"next_step\": \"end\", \"final_summary\": f\"오류: 이력서 분석 실패 (LLM 오류) - {e}\"}\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"resume_summary\": result.get(\"resume_summary\", \"\"),\n",
        "        \"resume_keywords\": result.get(\"resume_keywords\", []),\n",
        "        \"target_company_keywords\": result.get(\"target_company_keywords\", []),\n",
        "        \"next_step\": \"generate_question_strategy\"\n",
        "    }\n",
        "\n",
        "# 4) 질문 전략 수립 (고도화 버전)\n",
        "def generate_question_strategy(state: InterviewState) -> InterviewState:\n",
        "    resume_summary = state.get('resume_summary')\n",
        "    resume_keywords = state.get('resume_keywords')\n",
        "    target_company_keywords = state.get('target_company_keywords')\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    parser = JsonOutputParser()\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    당신은 기업 면접관입니다. 이력서 요약과 키워드를 참고하여 면접 질문 전략을 수립하세요.\n",
        "    [이력서 요약] {resume_summary}\n",
        "    [지원자 키워드] {resume_keywords}\n",
        "    [회사 키워드] {target_company_keywords}\n",
        "\n",
        "    면접 질문 전략은 다음 3가지 분야로 구성합니다:\n",
        "    1. 경력과 경험 (지원자의 과거 경험과 직무 역량 검증)\n",
        "    2. 커뮤니케이션 능력 (팀워크, 협업, 갈등 해결 능력 검증)\n",
        "    3. 문제해결력 (돌발 상황, 기술적 난관 해결 방식 검증)\n",
        "\n",
        "    각 분야별로 아래 JSON 형식으로 작성하세요:\n",
        "    {{\n",
        "      \"strategy_summary\": \"전체 전략 요약 (2~3문장)\",\n",
        "      \"categories\": [\n",
        "        {{\n",
        "          \"category\": \"경력과 경험\",\n",
        "          \"question_direction\": \"면접관이 어떤 관점으로 질문을 이끌어갈지에 대한 설명\",\n",
        "          \"example_question\": [\"예시 질문 1\", \"예시 질문 2\"]\n",
        "        }},\n",
        "        {{\n",
        "          \"category\": \"커뮤니케이션 능력\",\n",
        "          \"question_direction\": \"면접관이 확인하려는 의도와 평가 포인트\",\n",
        "          \"example_question\": [\"예시 질문 1\", \"예시 질문 2\"]\n",
        "        }},\n",
        "        {{\n",
        "          \"category\": \"문제해결력\",\n",
        "          \"question_direction\": \"지원자의 판단력, 책임감 등을 평가하기 위한 방향\",\n",
        "          \"example_question\": [\"예시 질문 1\", \"예시 질문 2\"]\n",
        "        }}\n",
        "      ]\n",
        "    }}\n",
        "    \"\"\")\n",
        "    chain = prompt | llm | parser\n",
        "    try:\n",
        "        result = chain.invoke({\n",
        "            \"resume_summary\": resume_summary,\n",
        "            \"resume_keywords\": resume_keywords,\n",
        "            \"target_company_keywords\": target_company_keywords\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return {**state, \"next_step\": \"end\", \"final_summary\": f\"오류: 질문 전략 수립 실패 (LLM 오류) - {e}\"}\n",
        "\n",
        "    strategy_dict = {\n",
        "        \"summary\": result.get(\"strategy_summary\", \"\"),\n",
        "        \"categories\": result.get(\"categories\", [])\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"question_strategy\": strategy_dict,\n",
        "        \"next_step\": \"initialize_queues\"\n",
        "    }\n",
        "\n",
        "# 5) 답변 입력 (고도화 버전)\n",
        "def update_current_answer(state: InterviewState) -> InterviewState:\n",
        "    return {\n",
        "        **state,\n",
        "        \"next_step\": \"evaluate_answer\"\n",
        "    }\n",
        "\n",
        "# 6) 답변 평가 (고도화 버전)\n",
        "def evaluate_answer(state: dict) -> dict:\n",
        "    current_question = state.get(\"current_question\")\n",
        "    current_answer = state.get(\"current_answer\")\n",
        "    conversation = state.get(\"conversation\", [])\n",
        "    evaluation = state.get(\"evaluation\", [])\n",
        "\n",
        "    if not current_question or not current_answer:\n",
        "        return {\n",
        "            **state,\n",
        "            \"next_step\": \"summarize\",\n",
        "            \"final_summary\": \"오류: 지원자가 응답하지 않아 면접을 종료합니다.\"\n",
        "        }\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    parser = JsonOutputParser()\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"당신은 AI 면접관의 답변 평가 전문가입니다.\n",
        "        질문과 답변을 바탕으로 아래 기준을 0~100점으로 채점하고, 가중치를 적용한 총점을 계산하세요.\n",
        "        [평가 기준 및 가중치]\n",
        "        - 연관성 (0.30), 구체성 (0.20), 영향/성과 (0.20), 역할·주도성 (0.20), 커뮤니케이션(명료성) (0.10)\n",
        "        [출력 형식(JSON)]\n",
        "        {{\n",
        "          \"기준\": {{\n",
        "            \"연관성\": {{ \"점수\": <0~100 정수>, \"근거\": \"<문장>\" }},\n",
        "            \"구체성\": {{ \"점수\": <0~100 정수>, \"근거\": \"<문장>\" }},\n",
        "            \"영향/성과\": {{ \"점수\": <0~100 정수>, \"근거\": \"<문장>\" }},\n",
        "            \"역할·주도성\": {{ \"점수\": <0~100 정수>, \"근거\": \"<문장>\" }},\n",
        "            \"커뮤니케이션(명료성)\": {{ \"점수\": <0~100 정수>, \"근거\": \"<문장>\" }}\n",
        "          }},\n",
        "          \"가중합_점수\": <0~100 정수>,\n",
        "          \"등급\": \"<상|중|하>\",\n",
        "          \"핵심피드백\": \"<1~2문장 요점>\",\n",
        "          \"개선가이드\": [\"<실행 팁 1>\", \"<실행 팁 2>\"]\n",
        "        }}\n",
        "        [등급 기준] (81-100: 상), (51-80: 중), (0-50: 하)\n",
        "        ---\n",
        "        [질문]: {question}\n",
        "        [답변]: {answer}\n",
        "        \"\"\"\n",
        "    )\n",
        "    chain = prompt | llm | parser\n",
        "\n",
        "    try:\n",
        "        eval_result = chain.invoke({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": current_answer,\n",
        "        })\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE: print(f\"❌ [DEBUG] evaluate_answer 오류: {e}\")\n",
        "        eval_result = {}\n",
        "\n",
        "    conversation.append({\"question\": current_question, \"answer\": current_answer})\n",
        "    evaluation.append({\n",
        "        \"question\": current_question,\n",
        "        \"answer\": current_answer,\n",
        "        \"evaluation\": eval_result,\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"conversation\": conversation,\n",
        "        \"evaluation\": evaluation,\n",
        "        \"last_evaluation\": eval_result,\n",
        "        \"next_step\": \"reflect\",\n",
        "    }\n",
        "\n",
        "# 7) 평가 검증 (Reflection) (고도화 버전)\n",
        "def reflect(state: InterviewState) -> InterviewState:\n",
        "    current_question = state.get(\"current_question\", \"\")\n",
        "    current_answer = state.get(\"current_answer\", \"\")\n",
        "    last_evaluation = state.get(\"last_evaluation\", None)\n",
        "\n",
        "    if not last_evaluation:\n",
        "        return {\n",
        "            **state,\n",
        "            \"reflection_status\": \"재평가 필요\",\n",
        "            \"reflection_detail\": { \"사유\": \"1차 평가(E) 없음\" },\n",
        "            \"next_step\": \"re_evaluate_answer\",\n",
        "        }\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    parser = JsonOutputParser()\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"당신은 \"감수관\"입니다. Q에 대한 A를 평가한 E가 적절했는지 평가합니다.\n",
        "        [출력(JSON)]\n",
        "        {{\n",
        "          \"평가_적절성_점수\": (1~100),\n",
        "          \"판정\": \"적절 | 부분 부적절 | 부적절\",\n",
        "          \"핵심사유\": \"\",\n",
        "          \"과대과소_판정\": \"과대 | 과소 | 중립\",\n",
        "          \"권장_수정\": {{ \"평가_총점\": (1~100), \"평가_코멘트\": \"E가 따를 보완 지시\" }}\n",
        "        }}\n",
        "        --- Q(질문) ---\n",
        "        {question}\n",
        "        --- A(답변) ---\n",
        "        {answer}\n",
        "        --- E(LLM의 평가, JSON) ---\n",
        "        {evaluation}\n",
        "        \"\"\"\n",
        "    )\n",
        "    chain = prompt | llm | parser\n",
        "    eval_str = json.dumps(last_evaluation, ensure_ascii=False)\n",
        "\n",
        "    try:\n",
        "        result = chain.invoke({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": current_answer,\n",
        "            \"evaluation\": eval_str\n",
        "        })\n",
        "        adequacy = int(result.get(\"평가_적절성_점수\", 0))\n",
        "        verdict = (result.get(\"판정\") or \"\").strip()\n",
        "        is_ok = (adequacy >= 70) or (verdict == \"적절\")\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE: print(f\"❌ [DEBUG] reflect 오류: {e}\")\n",
        "        is_ok = False\n",
        "        result = {\"사유\": f\"Reflection 오류: {e}\"}\n",
        "\n",
        "\n",
        "    if not is_ok:\n",
        "        return {\n",
        "            **state,\n",
        "            \"reflection_status\": \"재평가 필요\",\n",
        "            \"reflection_detail\": result,\n",
        "            \"next_step\": \"re_evaluate_answer\"\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"reflection_status\": \"정상\",\n",
        "        \"reflection_detail\": result,\n",
        "        \"next_step\": \"decide_next_step\"\n",
        "    }\n",
        "\n",
        "# 8) 답변 재평가 (고도화 버전)\n",
        "def re_evaluate_answer(state: InterviewState) -> InterviewState:\n",
        "    current_question = state.get(\"current_question\")\n",
        "    current_answer = state.get(\"current_answer\")\n",
        "    evaluation = state.get(\"evaluation\", [])\n",
        "    reflection_detail = state.get(\"reflection_detail\", {})\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    parser = JsonOutputParser()\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"당신은 AI 면접관의 재평가자입니다.\n",
        "        질문과 답변을 바탕으로 아래 기준을 0~100점으로 채점하고, 가중치를 적용한 총점을 계산하세요.\n",
        "        단, 아래의 감수 리포트를 반영하세요.\n",
        "\n",
        "        [평가 기준 및 가중치]\n",
        "        - 연관성 (0.30), 구체성 (0.20), 영향/성과 (0.20), 역할·주도성 (0.20), 커뮤니케이션(명료성) (0.10)\n",
        "\n",
        "        [감수 리포트(반영할 지적사항)]\n",
        "        {reflection_detail}\n",
        "\n",
        "        [출력 형식(JSON)]\n",
        "        {{\n",
        "          \"기준\": {{ ... (evaluate_answer와 동일) ... }},\n",
        "          \"가중합_점수\": <0~100 정수>,\n",
        "          \"등급\": \"<상|중|하>\",\n",
        "          \"핵심피드백\": \"<1~2문장 요점>\",\n",
        "          \"개선가이드\": [\"<실행 팁 1>\", \"<실행 팁 2>\"]\n",
        "        }}\n",
        "\n",
        "        [등급 기준] (81-100: 상), (51-80: 중), (0-50: 하)\n",
        "        ---\n",
        "        [질문]: {question}\n",
        "        [답변]: {answer}\n",
        "        \"\"\"\n",
        "    )\n",
        "    chain = prompt | llm | parser\n",
        "\n",
        "    try:\n",
        "        eval_result = chain.invoke({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": current_answer,\n",
        "            \"reflection_detail\": json.dumps(reflection_detail, ensure_ascii=False)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE: print(f\"❌ [DEBUG] re_evaluate_answer 오류: {e}\")\n",
        "        eval_result = {}\n",
        "\n",
        "    if evaluation and evaluation[-1].get(\"question\") == current_question:\n",
        "        evaluation[-1][\"evaluation\"] = eval_result\n",
        "    else:\n",
        "        evaluation.append({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": current_answer,\n",
        "            \"evaluation\": eval_result,\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"evaluation\": evaluation,\n",
        "        \"last_evaluation\": eval_result,\n",
        "        \"next_step\": \"decide_next_step\"\n",
        "    }\n",
        "\n",
        "# 9) 다음 질문 큐에서 가져오기 (고도화 버전)\n",
        "def get_next_question_from_queue(state: InterviewState) -> InterviewState:\n",
        "    question_queue = state.get(\"question_queue\", [])\n",
        "    if not question_queue:\n",
        "        return {**state, \"next_step\": \"next_topic\"}\n",
        "\n",
        "    next_question = question_queue.pop(0)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_question\": next_question,\n",
        "        \"question_queue\": question_queue,\n",
        "        \"current_answer\": \"\",\n",
        "        \"generate_count\": \"0\",\n",
        "        \"next_step\": \"END\"\n",
        "    }\n",
        "\n",
        "# 10) 다음 주제 질문 가져오기 (고도화 버전)\n",
        "def next_topic_question(state: InterviewState) -> InterviewState:\n",
        "    remaining_topics = state.get(\"remaining_topics\", [])\n",
        "    topic_map = state.get(\"topic_map\", {})\n",
        "\n",
        "    if not remaining_topics:\n",
        "        return {**state, \"next_step\": \"end\"}\n",
        "\n",
        "    current_topic = remaining_topics.pop(0)\n",
        "    question_queue = list(topic_map.get(current_topic, [])) # ⭐️ list()로 복사\n",
        "\n",
        "    if not question_queue:\n",
        "        return {\n",
        "            **state,\n",
        "            \"current_strategy\": current_topic,\n",
        "            \"remaining_topics\": remaining_topics,\n",
        "            \"question_queue\": [],\n",
        "            \"generate_count\": \"0\",\n",
        "            \"next_step\": \"generate\"\n",
        "        }\n",
        "\n",
        "    selected_question = question_queue.pop(0)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_question\": selected_question,\n",
        "        \"current_strategy\": current_topic,\n",
        "        \"remaining_topics\": remaining_topics,\n",
        "        \"question_queue\": question_queue,\n",
        "        \"current_answer\": \"\",\n",
        "        \"generate_count\": \"0\",\n",
        "        \"next_step\": \"END\"\n",
        "    }\n",
        "\n",
        "# 11) 인터뷰 진행 검토 (고도화 버전 - 규칙 기반)\n",
        "def decide_next_step(state: InterviewState) -> InterviewState:\n",
        "    conversation_count = len(state.get(\"conversation\", []))\n",
        "    last_evaluation = state.get(\"last_evaluation\", {})\n",
        "    question_queue = state.get(\"question_queue\", [])\n",
        "    remaining_topics = state.get(\"remaining_topics\", [])\n",
        "\n",
        "    if conversation_count >= 6:\n",
        "        return {**state, \"next_step\": \"end\"}\n",
        "\n",
        "    last_grade = last_evaluation.get(\"등급\", \"중\")\n",
        "    if last_grade == \"하\":\n",
        "        return {**state, \"next_step\": \"generate\"}\n",
        "\n",
        "    if question_queue:\n",
        "        return {**state, \"next_step\": \"next_question\"}\n",
        "\n",
        "    if not question_queue and remaining_topics:\n",
        "        return {**state, \"next_step\": \"next_topic\"}\n",
        "\n",
        "    return {**state, \"next_step\": \"end\"}\n",
        "\n",
        "\n",
        "# 12) 질문 생성 (심화질문) (고도화 버전 - Chroma DB 연동)\n",
        "def generate_question(state: InterviewState) -> InterviewState:\n",
        "    resume_summary = state.get(\"resume_summary\", \"\")\n",
        "    resume_keywords = state.get(\"resume_keywords\", [])\n",
        "    current_strategy = state.get(\"current_strategy\", \"\")\n",
        "    conversation = state.get(\"conversation\", [])\n",
        "    generate_count = int(state.get(\"generate_count\", 0))\n",
        "\n",
        "    history_str = \"\"\n",
        "    for i, conv in enumerate(conversation):\n",
        "        history_str += f\"\\nQ{i+1}: {conv['question']}\\nA{i+1}: {conv['answer']}\\n\"\n",
        "    if not history_str: history_str = \"아직 면접이 시작되지 않았습니다.\"\n",
        "\n",
        "    if generate_count > 1:\n",
        "        return {\n",
        "            **state,\n",
        "            \"next_step\": \"next_topic\",\n",
        "            \"generate_count\": \"0\"\n",
        "        }\n",
        "\n",
        "    depth_comment = \"이전 답변을 바탕으로, 지원자의 역량을 더 깊이 검증할 수 있는 심화 질문을 생성하세요.\"\n",
        "\n",
        "    # Chroma Vector DB 연동\n",
        "    try:\n",
        "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "        db_path = \"./chroma_db\"\n",
        "        if not os.path.exists(db_path):\n",
        "            os.makedirs(db_path)\n",
        "            db = Chroma.from_texts(texts=[\"샘플 질문 1\"], embedding=embeddings, persist_directory=db_path)\n",
        "        else:\n",
        "            db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "\n",
        "        query_text = f\"{resume_summary}\\n{history_str}\"\n",
        "        similar_docs = db.similarity_search(query_text, k=3)\n",
        "        similar_questions = \"\\n\".join([f\"- {doc.page_content}\" for doc in similar_docs])\n",
        "    except Exception as e:\n",
        "        similar_questions = \"유사 질문 정보를 불러올 수 없습니다.\"\n",
        "        if DEBUG_MODE: print(f\"[Chroma 검색 오류] {e}\")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"당신은 전문 AI 면접관입니다.\n",
        "        아래 정보를 참고하여 지원자에게 **새로운 심층 질문**을 하나만 생성하세요.\n",
        "        [참고: 기존 면접 질문]\n",
        "        {similar_questions}\n",
        "        [규칙]\n",
        "        1. 위 유사 질문들과 **겹치지 않는 새로운 질문**을 만드세요.\n",
        "        2. 지원자의 이전 답변을 바탕으로 사고력, 문제해결력, 가치관을 탐색하세요.\n",
        "        3. 질문은 한 문장으로, 명확하고 간결하게 작성하세요.\n",
        "        4. {depth_comment}\n",
        "        ---\n",
        "        [지원자 이력서 요약] {summary}\n",
        "        [현재 면접 주제] {strategy}\n",
        "        [지금까지의 면접 기록] {history}\n",
        "        ---\n",
        "        [다음 심층 질문 (오직 한 문장)]:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"summary\": resume_summary,\n",
        "        \"strategy\": current_strategy,\n",
        "        \"history\": history_str,\n",
        "        \"depth_comment\": depth_comment,\n",
        "        \"similar_questions\": similar_questions\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"current_question\": response.strip(),\n",
        "        \"current_answer\": \"\",\n",
        "        \"generate_count\": str(generate_count + 1),\n",
        "        \"next_step\": \"END\"\n",
        "    }\n",
        "\n",
        "# 13) 인터뷰 피드백 보고서 (⭐️⭐️⭐️ 버그 수정 ⭐️⭐️⭐️)\n",
        "def summarize_interview(state: InterviewState) -> InterviewState:\n",
        "    report_lines = []\n",
        "    report_lines.append(\"== AI 면접 최종 피드백 보고서 ==\")\n",
        "\n",
        "    report_lines.append(\"\\n[이력서 요약]\")\n",
        "    report_lines.append(state.get(\"resume_summary\", \"요약 정보 없음\"))\n",
        "\n",
        "    conversation = state.get(\"conversation\", [])\n",
        "    evaluation = state.get(\"evaluation\", [])\n",
        "\n",
        "    if not conversation or not evaluation:\n",
        "        report_lines.append(\"\\n면접 기록이 없습니다.\")\n",
        "        return {**state, \"final_summary\": \"\\n\".join(report_lines), \"next_step\": \"END\"}\n",
        "\n",
        "    report_lines.append(\"\\n== 면접 상세 내역 ==\")\n",
        "    for i, (conv, eval_item) in enumerate(zip(conversation, evaluation), 1):\n",
        "        eval_data = eval_item.get(\"evaluation\", {})\n",
        "        criteria = eval_data.get(\"기준\", {})\n",
        "        report_lines.append(f\"\\n[질문 {i}] Q: {conv.get('question', 'N/A')}\")\n",
        "        report_lines.append(f\"A: {conv.get('answer', 'N/A')}\")\n",
        "        report_lines.append(f\"  [평가] 총점: {eval_data.get('가중합_점수', 'N/A')} (등급: {eval_data.get('등급', 'N/A')})\")\n",
        "        report_lines.append(f\"  - 피드백: {eval_data.get('핵심피드백', 'N/A')}\")\n",
        "        report_lines.append(\"-\" * 40)\n",
        "\n",
        "    # 3. 종합 평가 (LLM)\n",
        "    report_lines.append(\"\\n== 종합 평가 ==\")\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "    # (⭐️⭐️⭐️ 핵심 수정 ⭐️⭐️⭐️)\n",
        "    # report_lines.index()는 \\n이 포함된 문자열을 찾아야 합니다.\n",
        "    try:\n",
        "        start_idx = report_lines.index(\"\\n== 면접 상세 내역 ==\")\n",
        "        interview_summary_text = \"\\n\".join(report_lines[start_idx:])\n",
        "    except ValueError:\n",
        "        interview_summary_text = \"\\n\".join(report_lines) # 못찾으면 전체 전송\n",
        "\n",
        "    comprehensive_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"당신은 전문 채용 컨설턴트입니다.\n",
        "        다음 면접 기록을 바탕으로 **구체적이고 실질적인** 종합 평가를 작성하세요.\n",
        "        **면접 기록:**\n",
        "        {interview_text}\n",
        "        **다음 형식으로 구체적으로 작성하세요:**\n",
        "        1. 전체 인상\n",
        "        2. 강점 (2-3가지)\n",
        "        3. 약점 (2-3가지)\n",
        "        4. 개선 방안 (2-3가지)\n",
        "        \"\"\"\n",
        "    )\n",
        "    comprehensive_chain = comprehensive_prompt | llm | StrOutputParser()\n",
        "    try:\n",
        "        comprehensive_feedback = comprehensive_chain.invoke({\n",
        "            \"interview_text\": interview_summary_text\n",
        "        })\n",
        "        report_lines.append(comprehensive_feedback)\n",
        "    except Exception as e:\n",
        "        report_lines.append(f\"종합 평가 생성 중 오류 발생: {e}\")\n",
        "\n",
        "    # 4. 합격 가능성 분석 (규칙 기반)\n",
        "    report_lines.append(\"\\n== 모의 합격 가능성 분석 ==\")\n",
        "    total_score, max_score, good_count, poor_count = 0, 0, 0, 0\n",
        "\n",
        "    for eval_item in evaluation:\n",
        "        eval_dict = eval_item.get(\"evaluation\", {})\n",
        "        grade = eval_dict.get(\"등급\", \"하\")\n",
        "        if grade == \"상\": total_score += 3; good_count += 1\n",
        "        elif grade == \"중\": total_score += 2; good_count += 1\n",
        "        else: total_score += 1; poor_count += 1\n",
        "        max_score += 3\n",
        "\n",
        "    base_pass_rate = (total_score / max_score * 100) if max_score > 0 else 0\n",
        "    count_bonus = min(len(conversation) * 5, 20)\n",
        "    final_pass_rate = min(base_pass_rate + count_bonus, 95)\n",
        "\n",
        "    if final_pass_rate >= 85: possibility, probability, comment = \"매우 높음\", \"85-95%\", \"우수한 답변을 하였고, 충분히 검증되었습니다.\"\n",
        "    elif final_pass_rate >= 70: possibility, probability, comment = \"높음\", \"70-85%\", \"전반적으로 좋은 평가를 받았습니다.\"\n",
        "    elif final_pass_rate >= 55: possibility, probability, comment = \"보통\", \"50-70%\", \"기본은 갖췄으나, 보완이 필요합니다.\"\n",
        "    else: possibility, probability, comment = \"낮음\", \"50% 미만\", \"여러 영역에서 개선이 필요합니다.\"\n",
        "\n",
        "    report_lines.append(f\"[합격 가능성] {possibility} ({probability})\")\n",
        "    report_lines.append(f\"(기본 {base_pass_rate:.1f}% + 면접 보너스 {count_bonus}% = {final_pass_rate:.1f}%)\")\n",
        "    report_lines.append(f\"{comment}\")\n",
        "    report_lines.append(\"\\n== 면접 종료 ==\")\n",
        "\n",
        "    final_report_str = \"\\n\".join(report_lines)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"final_summary\": final_report_str,\n",
        "        \"next_step\": \"END\"\n",
        "    }\n",
        "\n",
        "\n",
        "####### 4. LangGraph 정의 (Gradio 호환) #######\n",
        "\n",
        "builder = StateGraph(InterviewState)\n",
        "\n",
        "builder.add_node(\"evaluate\", evaluate_answer)\n",
        "builder.add_node(\"reflection\", reflect)\n",
        "builder.add_node(\"re_evaluate_answer\", re_evaluate_answer)\n",
        "builder.add_node(\"decide_next\", decide_next_step)\n",
        "builder.add_node(\"generate_question\", generate_question)\n",
        "builder.add_node(\"get_next_question_from_queue\", get_next_question_from_queue)\n",
        "builder.add_node(\"next_topic_question\", next_topic_question)\n",
        "builder.add_node(\"summarize\", summarize_interview)\n",
        "\n",
        "builder.set_entry_point(\"evaluate\")\n",
        "builder.add_edge(\"evaluate\", \"reflection\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"reflection\",\n",
        "    lambda s: s.get(\"reflection_status\"),\n",
        "    { \"정상\": \"decide_next\", \"재평가 필요\": \"re_evaluate_answer\" }\n",
        ")\n",
        "builder.add_edge(\"re_evaluate_answer\", \"decide_next\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"decide_next\",\n",
        "    lambda s: s.get(\"next_step\"),\n",
        "    {\n",
        "        \"generate\": \"generate_question\",\n",
        "        \"next_question\": \"get_next_question_from_queue\",\n",
        "        \"next_topic\": \"next_topic_question\",\n",
        "        \"end\": \"summarize\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# ⭐️ (수정) generate_question 또는 next_topic_question이\n",
        "# 다음 주제(next_topic)로 가거나 끝(end)나야 할 때\n",
        "builder.add_conditional_edges(\n",
        "    \"generate_question\",\n",
        "    lambda s: s.get(\"next_step\"),\n",
        "    {\n",
        "        \"next_topic\": \"next_topic_question\", # 심화질문 2회 초과 시 다음 주제로\n",
        "        \"END\": END # 일반적인 심화질문 생성 시 Gradio 대기\n",
        "    }\n",
        ")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"next_topic_question\",\n",
        "    lambda s: s.get(\"next_step\"),\n",
        "    {\n",
        "        \"generate\": \"generate_question\",\n",
        "        \"END\": END,\n",
        "        \"end\": \"summarize\" # ⭐️ 남은 주제가 없을 때\n",
        "    }\n",
        ")\n",
        "\n",
        "builder.add_edge(\"get_next_question_from_queue\", END)\n",
        "builder.add_edge(\"summarize\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "print(\"✅ [PROD] 면접 Agent 그래프가 성공적으로 컴파일되었습니다!\")\n",
        "\n",
        "\n",
        "####### 5. Gradio 인터페이스 #######\n",
        "\n",
        "# 세션 상태 초기화 함수\n",
        "def initialize_state():\n",
        "    return {\n",
        "        \"state\": {\n",
        "            \"resume_text\": \"\", \"target_company\": \"\", \"resume_summary\": \"\",\n",
        "            \"resume_keywords\": [], \"target_company_keywords\": [],\n",
        "            \"question_strategy\": {}, \"current_question\": \"\", \"current_answer\": \"\",\n",
        "            \"current_strategy\": \"\", \"conversation\": [], \"evaluation\": [],\n",
        "            \"next_step\": \"\", \"generate_count\": \"0\", \"reflection_status\": \"\",\n",
        "            \"reflection_detail\": {}, \"last_evaluation\": {}, \"final_summary\": \"\",\n",
        "            \"question_queue\": [], \"remaining_topics\": [], \"topic_map\": {}\n",
        "        },\n",
        "        \"interview_started\": False,\n",
        "        \"interview_ended\": False,\n",
        "        \"chat_history\": []\n",
        "    }\n",
        "\n",
        "# 파일 업로드 후 인터뷰 초기화 (Gradio용으로 수정)\n",
        "def upload_and_initialize(resume_file, company_file, session_state):\n",
        "    if resume_file is None or company_file is None:\n",
        "        return session_state, [[\"🤖 AI 면접관\", \"이력서와 채용공고 파일을 모두 업로드해주세요.\"]], gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        state = initialize_state()[\"state\"]\n",
        "        state[\"resume_text\"] = extract_text_from_file(resume_file.name)\n",
        "        state[\"target_company\"] = extract_text_from_file(company_file.name)\n",
        "\n",
        "        state = analyze_resume(state)\n",
        "        if state[\"next_step\"] == \"end\":\n",
        "             return session_state, [[\"🤖 AI 면접관\", state[\"final_summary\"]]], gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "        state = generate_question_strategy(state)\n",
        "\n",
        "        categories = state[\"question_strategy\"].get(\"categories\", [])\n",
        "        topic_map = {cat.get(\"category\"): cat.get(\"example_question\", []) for cat in categories if cat.get(\"category\")}\n",
        "        remaining_topics = [cat.get(\"category\") for cat in categories if cat.get(\"category\")]\n",
        "\n",
        "        if not remaining_topics:\n",
        "             return session_state, [[\"🤖 AI 면접관\", \"질문 전략 수립에 실패했습니다.\"]], gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "        first_topic = remaining_topics.pop(0)\n",
        "        question_queue = list(topic_map.get(first_topic, [])) # ⭐️ list()로 복사\n",
        "\n",
        "        if not question_queue:\n",
        "            first_question = f\"{first_topic}에 대해 자유롭게 말씀해 주세요.\"\n",
        "        else:\n",
        "            first_question = question_queue.pop(0)\n",
        "\n",
        "        state[\"current_question\"] = first_question\n",
        "        state[\"current_strategy\"] = first_topic\n",
        "        state[\"question_queue\"] = question_queue\n",
        "        state[\"remaining_topics\"] = remaining_topics\n",
        "        state[\"topic_map\"] = topic_map\n",
        "        state[\"next_step\"] = \"answer\"\n",
        "\n",
        "        session_state[\"state\"] = state\n",
        "        session_state[\"interview_started\"] = True\n",
        "        session_state[\"chat_history\"] = [[\"🤖 AI 면접관\", \"준비가 완료되었습니다. 첫 번째 질문입니다.\" + \"\\n\\n\" + first_question]]\n",
        "\n",
        "        return session_state, session_state[\"chat_history\"], gr.update(visible=False), gr.update(visible=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        return session_state, [[\"🤖 AI 면접관\", f\"오류 발생: {e}\"]], gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "\n",
        "# 답변 처리 및 다음 질문 생성 (⭐️⭐️⭐️ 버그 수정 ⭐️⭐️⭐️)\n",
        "def chat_interview(user_input, session_state):\n",
        "    if not session_state[\"interview_started\"] or session_state[\"interview_ended\"]:\n",
        "        return session_state, session_state[\"chat_history\"], gr.update(value=\"\")\n",
        "\n",
        "    if not user_input or user_input.strip() == \"\":\n",
        "        session_state[\"chat_history\"].append([\"🤖 AI 면접관\", \"답변이 비어있습니다. 답변을 입력해주세요.\"])\n",
        "        return session_state, session_state[\"chat_history\"], gr.update(value=\"\")\n",
        "\n",
        "    # (1) 사용자 답변 저장\n",
        "    session_state[\"chat_history\"].append([\"🙋‍♂️ 지원자\", user_input])\n",
        "\n",
        "    # (2) State에 답변 업데이트\n",
        "    state = session_state[\"state\"]\n",
        "    state[\"current_answer\"] = user_input.strip()\n",
        "    state[\"next_step\"] = \"evaluate\"\n",
        "\n",
        "    # (3) Agent 그래프 실행 (evaluate ~ END)\n",
        "    try:\n",
        "        if DEBUG_MODE: print(f\"\\n--- 🚀 graph.invoke() 직전 (Next Step: {state['next_step']}) ---\")\n",
        "\n",
        "        # ⭐️⭐️⭐️ 핵심 수정 ⭐️⭐️⭐️\n",
        "        # state를 받아 state를 반환하는 'graph.invoke()'는\n",
        "        # state 딕셔너리 *전체*를 입력으로 받아야 합니다.\n",
        "        state = graph.invoke(state)\n",
        "\n",
        "        if DEBUG_MODE: print(f\"\\n--- 🏁 graph.invoke() 직후 (Next Step: {state['next_step']}) ---\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE: print(f\"❌ [DEBUG] graph.invoke 오류: {e}\")\n",
        "        state[\"next_step\"] = \"end\"\n",
        "        state[\"final_summary\"] = f\"면접 중 오류가 발생하여 종료합니다. (오류: {e})\"\n",
        "        # 오류 발생 시에도 세션 상태를 업데이트해야 합니다.\n",
        "        session_state[\"state\"] = state\n",
        "\n",
        "\n",
        "    # (4) Gradio 세션 상태 업데이트\n",
        "    session_state[\"state\"] = state\n",
        "\n",
        "    # (5) 종료 여부 판단 (final_summary가 채워졌는지 확인)\n",
        "    if state.get(\"final_summary\"):\n",
        "        session_state[\"interview_ended\"] = True\n",
        "        final_summary = state.get(\"final_summary\")\n",
        "        session_state[\"chat_history\"].append([\"🤖 AI 면접관\", final_summary])\n",
        "\n",
        "        return session_state, session_state[\"chat_history\"], gr.update(value=\"면접 종료됨\", interactive=False)\n",
        "\n",
        "    # (6) 다음 질문 진행\n",
        "    # ⭐️⭐️⭐️ 핵심 수정 ⭐️⭐️⭐️\n",
        "    # state['next_step']이 'next_topic' 또는 'generate'일 수 있습니다. (심화질문 2회 초과 / 큐가 빈 경우)\n",
        "    # 이 경우, 그래프를 다시 호출하여 다음 질문을 가져와야 합니다.\n",
        "    elif state[\"next_step\"] in [\"next_topic\", \"generate\"]:\n",
        "        if DEBUG_MODE: print(f\"    -> 다음 턴 자동 실행: {state['next_step']}\")\n",
        "        # state['current_answer']를 비워야 evaluate에서 막히지 않습니다.\n",
        "        state[\"current_answer\"] = \"\"\n",
        "        # chat_interview를 재귀적으로 호출하여 다음 질문을 바로 받아옵니다.\n",
        "        # (user_input은 \"Auto-Trigger\"와 같은 임시 값으로 채워도 무방합니다)\n",
        "        return chat_interview(\" \", session_state)\n",
        "\n",
        "    else:\n",
        "        # (next_step == \"END\" 상태)\n",
        "        # 다음 질문 표시 (generate, next_question, next_topic에서 생성됨)\n",
        "        next_question = state[\"current_question\"]\n",
        "        session_state[\"chat_history\"].append([\"🤖 AI 면접관\", next_question])\n",
        "        return session_state, session_state[\"chat_history\"], gr.update(value=\"\")\n",
        "\n",
        "# (Gradio) 재시작 함수\n",
        "def restart_interview():\n",
        "    return initialize_state(), [], gr.update(visible=True), gr.update(visible=False), gr.update(value=\"\", interactive=True)\n",
        "\n",
        "# Gradio 인터페이스 구성 (2개 파일 입력으로 수정)\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    session_state = gr.State(initialize_state)\n",
        "\n",
        "    gr.Markdown(\"# 🤖 AI 면접관 (고도화 v2.5)\")\n",
        "    gr.Markdown(\"이력서와 지원하는 포지션의 채용공고(JD)를 업로드하고 인터뷰를 시작하세요.\")\n",
        "\n",
        "    with gr.Group(visible=True) as upload_group:\n",
        "        with gr.Row():\n",
        "            resume_file = gr.File(label=\"1. 이력서 업로드 (PDF 또는 DOCX)\")\n",
        "            company_file = gr.File(label=\"2. 채용공고 업로드 (PDF 또는 DOCX)\")\n",
        "        upload_btn = gr.Button(\"인터뷰 시작\", variant=\"primary\")\n",
        "\n",
        "    with gr.Group(visible=False) as chat_group:\n",
        "        chatbot = gr.Chatbot(label=\"AI 면접관\", height=500, show_copy_button=True, bubble_full_width=True)\n",
        "        user_input = gr.Textbox(show_label=False, placeholder=\"답변을 입력하고 Enter를 누르세요.\")\n",
        "        restart_btn = gr.Button(\"처음부터 다시 시작\")\n",
        "\n",
        "    upload_btn.click(\n",
        "        upload_and_initialize,\n",
        "        inputs=[resume_file, company_file, session_state],\n",
        "        outputs=[session_state, chatbot, upload_group, chat_group]\n",
        "    )\n",
        "\n",
        "    user_input.submit(\n",
        "        chat_interview,\n",
        "        inputs=[user_input, session_state],\n",
        "        outputs=[session_state, chatbot, user_input]\n",
        "    )\n",
        "\n",
        "    restart_btn.click(\n",
        "        restart_interview,\n",
        "        inputs=[],\n",
        "        outputs=[session_state, chatbot, upload_group, chat_group, user_input]\n",
        "    )\n",
        "\n",
        "\n",
        "# 실행\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "VvYpBoXHZfvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2d1387-71cc-418e-cc80-48f20d8e3fff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. 실행**"
      ],
      "metadata": {
        "id": "vVw8pSQRyy73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "JRnTUFUs_1f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86253cd-685f-4df3-99ba-312d4284e8c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key 로드 성공.\n",
            "✅ [PROD] 면접 Agent 그래프가 성공적으로 컴파일되었습니다!\n",
            "/content/app.py:870: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"AI 면접관\", height=500, show_copy_button=True, bubble_full_width=True)\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://6b4a008533fb9544b2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "/content/app.py:511: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
            "/content/app.py:517: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2958, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 895, in <module>\n",
            "    demo.launch(share=True, debug=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2865, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2962, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1153, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6b4a008533fb9544b2.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MTnQq6qKWLi"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}